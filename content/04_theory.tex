%theory
\chapter{Point Source Searches in IceCube} \label{sec:theory}

This chapter provides an insight into the theoretical formulas, principles and softwares used to perform a point source analysis.
The theoretical background gives a basic impression of the calculation and is necessary for understanding chapter \ref{sec:tdepps}, where the general calculation of the test statistic is discussed.
\section{Modelling the Test Statistic}

Basically, a point source search consists of a hypothesis test based on two hypotheses \cite{likelihood_method}.
The null hypothesis $H_0$ states that the examined data consist only of atmospheric background, while the hypothesis $H_\text{S}$ states that the data consist of background and astrophysical neutrino events, the latter emitted by a source of certain properties.
The hypothesis $H_\text{S}$ uses the parameterspace $\Theta$ while the null hypothesis only uses a subset of this parameterspace $\Theta_0$
The hypothesis test is the quotient of the underlying likelihoods $\CL$ of the hypotheses,
\begin{equation}
  \Lambda = \frac{\CL_{\Theta_0}}{\CL_\Theta}.
\end{equation}
Since the hypothesis $H_\text{S}$ contains the null hypothesis, the test statistic $TS$ can be expressed via Wilks' theorem \cite{wilk} as
\begin{equation}
  TS = -2\ln\hat{\Lambda} = 2\ln\Gb{\sup_{\theta\in\Theta}\CL(\theta)} - 2\ln\Gb{\sup_{\theta\in\Theta_0}\CL(\theta)}, \label{eq:TS}
\end{equation}
with the set $\theta$ of parameters that maximizes the likelihoods.
The test statistic now follows a $\chi^2$ distribution, given that the distribution is large enough.
In general, the likelihoods are defined as
\begin{equation}
  \CL(\lambda) = \frac{\lambda^Ne^{-\lambda}}{N!}\prod^N_{i=1}P_i
\end{equation}
or
\begin{equation}
  \ln\CL(\lambda) = -\lambda + \sum^N_{i=1}\ln{(\lambda P_i)}
\end{equation}
in the the logarithmic form \cite{ex_likelihood} dropping the $\ln(N!)$ term, where $\lambda$ is the number of expected events and $N$ is the total number of individual events $i$, $P_i$ defines the per-event model distributions.
In the case of stacking several sources $k$, the number of expected events is divided and the formula
\begin{equation}
  \ln\CL(\{\lambda_k\}) = -\sum^{N_\text{srcs}}_{k=1}\lambda_k + \sum^{N_\text{evts}}_{i=1}\ln{\Gb{\sum^{N_\text{srcs}}_{k=1}\lambda_k P_{i,k}}},
\end{equation}
is obtained.
If the likelihoods are inserted into formula \eqref{eq:TS}, the general formula for the test statistic is derived,
\begin{equation}
    TS = -2\ln{\hat\Lambda} = -2\sum_{k=1}^{N_{srcs}}\hat\lambda_{k,S} + 2\sum_{i=1}^{N_{\text{evts}}}\ln{\Gb{\frac{\sum_{k=1}^{N_{\text{srcs}}}\hat\lambda_{k,S}S_{i,k}}{\sum_{k=1}^{N_{\text{srcs}}}\langle\lambda_{k,B}\rangle B_{i,k}}+1}},
\end{equation}
with the $\hat\lambda$ maximizing the test statistic value.
The expected number of background events $\langle\lambda_{k,B}\rangle$ is estimated a priori.\\
The further form of the test statistic depends on the choice of probability models $S$ and $B$.
If a time-integrated search or a search in which the sources are independent of each other in terms of different temporal intervals is carried out, the formula simplifies to
\begin{equation}
    TS = -2\sum_{k=1}^{N_{\text{srcs}}}\hat\lambda_{k,S} + 2\sum_{i=1}^{N_{\text{evts}}}\sum_{k=1}^{N_{\text{srcs}}}\ln{\Gb{\frac{\hat\lambda_{k,S}S_{i,k}}{\langle\lambda_{k,B}\rangle B_{i,k}}+1}}.
\end{equation}
This makes it possible to form per-event PDF ratios directly in the calculation, which reduces the computational effort.\\
There is some software available to practically perform a point source search.
Among the best known are \textbf{sky\_llh}, \textbf{skylab} and \textbf{csky}. The software \textbf{csky}, which is used in this thesis, has a simple application and, due to the outsourcing of computationally intensive steps to \textbf{C++}, a time-efficient calculation.

\section{Likelihood in csky}

The likelihood of a source in csky looks like this,
\begin{equation}
  \CL(n_s,\Vmu)
  = \prod_i \Gb{
    \frac{n_s}{N} \CS(\Vx_i|\Vmu)
    + \Gp{1 - \frac{n_s}{N}} \CB(\Vx_i)
  }.
  \label{eq:lh}
\end{equation}

\begin{align}
  \CS(\Vx|\Vmu)
  &= \CS_S(\alpha,\delta,\sigma)
  \cdot \CS_E(E|\Vmu)
  \cdot \CS_T(t|\Vmu);
  \\
  \CB(\Vx)
  &= \CB_S(\delta)
  \cdot \CB_E(E)
  \cdot \CB_T(t).
  \label{eq:pdf}
\end{align}

mention fit parameters\\


\section{Calculating Sensitivity and Discovery Potential}

Sensitivity and discovery potential correspond in principle to the number of signal events that must be injected, so that the resulting test statistic is $\SI{90}{\percent}$ above the median of the background statistic or $\SI{50}{\percent}$ above $\num{5}\sigma$ of the latter respectively.
These conditions can be seen schematically in figure \ref{fig:sens_disc_schem}.
Let the number of sufficient signal events then be $N_\text{sig}$, the sensitivity $sens$ and discovery potential $disc p$ then are
\begin{align}
  sens &= \frac{N_\text{sig,sens}}{A_\text{det}}E_0, \\
  disc p &= \frac{N_\text{sig,disc p}}{A_\text{det}}E_0,
\end{align}
with the total signal acceptance of the detector $A_\text{det}$ and the reference energy $E_0$.
The time-dependent sensitivity and discovery potential are given in a fluence rather than a flux by multiplying with $E_0^2$, these results can then be compared to other analyses.
