%theory
\chapter{Point Source Searches in IceCube} \label{sec:theory}

This chapter provides an insight into the theoretical formulas, principles and softwares used to perform a point source analysis.
The theoretical background gives a basic impression of the calculation and is necessary for understanding chapter \ref{sec:tdepps}, where the general calculation of the test statistic is discussed.

\section{Modelling the Test Statistic}

Basically, a point source search consists of a hypothesis test based on two hypotheses \cite{likelihood_method}.
The null hypothesis $H_0$ states that the examined data consist only of atmospheric background, while the hypothesis $H_\text{S}$ states that the data consist of background and astrophysical neutrino events, the latter emitted by a source of certain properties.
The hypothesis $H_\text{S}$ uses the parameter space $\Theta$ while the null hypothesis only uses a subset of this parameterspace $\Theta_0$.
The hypothesis test is the quotient of the underlying likelihoods $\CL$ of the hypotheses,
\begin{equation}
  \Lambda = \frac{\CL_{\Theta_0}}{\CL_\Theta}.
\end{equation}
Since the hypothesis $H_\text{S}$ contains the null hypothesis, the test statistic $TS$ can be expressed via Wilks' theorem \cite{wilk} as
\begin{equation}
  TS = -2\ln\hat{\Lambda} = 2\ln\Gb{\sup_{\theta\in\Theta}\CL(\theta)} - 2\ln\Gb{\sup_{\theta\in\Theta_0}\CL(\theta)}, \label{eq:TS}
\end{equation}
with the set $\theta$ of parameters that maximizes the likelihoods.
The test statistic now follows a $\chi^2$ distribution, given that the sample size is large enough.
In point source searches, the likelihoods are defined as
\begin{equation}
  \CL(\lambda) = \frac{\lambda^Ne^{-\lambda}}{N!}\prod^N_{i=1}P_i
\end{equation}
or
\begin{equation}
  \ln\CL(\lambda) = -\lambda + \sum^N_{i=1}\ln{(\lambda P_i)}
\end{equation}
in the the logarithmic form \cite{ex_likelihood} dropping the $\ln(N!)$ term, where $\lambda$ is the number of expected events and $N$ is the total number of individual events $i$.
$P_i$ defines the per-event model distributions.
In the case of stacking several sources $k$, the number of expected events is divided and the formula
\begin{equation}
  \ln\CL(\{\lambda_k\}) = -\sum^{N_\text{srcs}}_{k=1}\lambda_k + \sum^{N_\text{evts}}_{i=1}\ln{\Gb{\sum^{N_\text{srcs}}_{k=1}\lambda_k P_{i,k}}},
\end{equation}
is obtained.
If the likelihoods are inserted into formula \eqref{eq:TS}, the general formula for the test statistic is derived,
\begin{equation}
    TS = -2\ln{\hat\Lambda} = -2\sum_{k=1}^{N_{srcs}}\hat\lambda_{k,S} + 2\sum_{i=1}^{N_{\text{evts}}}\ln{\Gb{\frac{\sum_{k=1}^{N_{\text{srcs}}}\hat\lambda_{k,S}S_{i,k}}{\sum_{k=1}^{N_{\text{srcs}}}\langle\lambda_{k,B}\rangle B_{i,k}}+1}},
\end{equation}
where $\hat\lambda$ maximizes the test statistic value.
The expected number of background events $\langle\lambda_{k,B}\rangle$ is estimated a priori integrated over the rate of background events.

The further form of the test statistic depends on the choice of probability models $S$ and $B$.
If a time-integrated search or a search in which the sources are independent of each other in terms of different temporal intervals is carried out, the formula simplifies to
\begin{equation}
    TS = -2\sum_{k=1}^{N_{\text{srcs}}}\hat\lambda_{k,S} + 2\sum_{i=1}^{N_{\text{evts}}}\sum_{k=1}^{N_{\text{srcs}}}\ln{\Gb{\frac{\hat\lambda_{k,S}S_{i,k}}{\langle\lambda_{k,B}\rangle B_{i,k}}+1}}.
\end{equation}
This makes it possible to form per-event probability density functions (PDF) ratios directly in the calculation, which reduces the computational effort.

There is some software available to practically perform a point source search.
Among the best known are \texttt{SkyLLH}\footnote{\url{https://github.com/icecube/skyllh}}, \texttt{skylab}\footnote{\url{https://github.com/coenders/skylab}} and \texttt{csky}\footnote{\url{https://github.com/icecube/csky}, private repository}.
The software \texttt{csky}, which is used in this thesis, is easy to use and, due to the outsourcing of computationally intensive steps to \texttt{C++}, a time-efficient calculation in comparison to the other mentioned softwares, which are implemented entirely in \texttt{Python}.

\section{Likelihood in csky}

The likelihood of a source in \texttt{csky} is shown in the equation below,
\begin{equation}
  \CL(n_s,\Vmu)
  = \prod_i \Gb{
    \frac{n_s}{N} \CS(\Vx_i|\Vmu)
    + \Gp{1 - \frac{n_s}{N}} \CB(\Vx_i)
  },
  \label{eq:lh}
\end{equation}
with the total number of events $N$, the event parameters $\Vx_i$ and the analysis parameters
\begin{align}
  \Vmu &= \{\text{number of signal events }n_S, \text{ spectral index }\gamma\},
\end{align}
where $\Vmu$ will be optimised\footnote{A representation of the likelihood can be found directly in the \texttt{csky} software package.}.
The probability functions of the individual events are broken down into three different parts according to the event parameters, an energy, a spatial and a temporal PDF.
However, the temporal PDF is only important for time-dependent searches,
\begin{align}
  \CS(\Vx|\Vmu)
  &= \CS_S(\delta,\alpha)
  \cdot \CS_E(E|\Vmu)
  \cdot \CS_T(t|\Vmu);
  \\
  \CB(\Vx)
  &= \CB_S(\delta)
  \cdot \CB_E(E)
  \cdot \CB_T(t).
  \label{eq:pdf}
\end{align}
The energy PDF can be constructed of a 2d histogram of energy and declination and is generated directly from the data for the background $\CB_E(E)$ and from simulations weighted with the spectral index $\gamma$ for the signal $\CS_E(E|\Vmu)$.
Its ratio does not depent on individual event properties and is calculated a priori.
The spatial PDF of the signal $\CS_S(\delta,\alpha)$ depends on the declination and right ascension of the event and corresponds to a Kent distribution around the position of the source.
The Kent distribution is the equivalent of a normal distribution on a sphere \cite{kent}.
The background PDF $\CB_S(\delta)$, on the other hand, is obtained directly from the spatial distribution of the data, since most events are background events.
It depends only on the declination, since the background is generally uniformly distributed in the sky and the distribution depends merely on the absorption of the earth.
The temporal PDF in case of this thesis is a simple box function, equal for both signal and background, that corresponds to $\CS_T(t|\Vmu)=1$ over a certain time interval.
Only events that fall within this time window make a contribution.

\section{Calculating Sensitivity and Discovery Potential}

Sensitivity and discovery potential are defined by the number of signal events that must be injected, so that the resulting test statistic is $\SI{90}{\percent}$ above the median of the background statistic or $\SI{50}{\percent}$ above $\num{5}\sigma$ of the latter respectively.
These conditions can be seen schematically in figure \ref{fig:sens_disc_schem}.
\begin{figure}\centering
  \begin{tikzpicture}[
declare function={gamma(\z)=
2.506628274631*sqrt(1/\z)+ 0.20888568*(1/\z)^(1.5)+ 0.00870357*(1/\z)^(2.5)- (174.2106599*(1/\z)^(3.5))/25920- (715.6423511*(1/\z)^(4.5))/1244160)*exp((-ln(1/\z)-1)*\z;},
declare function={gammapdf(\x,\k,\theta) = 1/(\theta^\k)*1/(gamma(\k))*\x^(\k-1)*exp(-\x/\theta);}]
\begin{axis}[ymode=log, ymin=0,
no markers, domain=0:9, samples=100,
axis lines=left, xlabel=$\text{TS}$, ylabel=$\#\text{trials}$,
y label style={at={(axis description cs:0.1,.5)},anchor=north},
x label style={dashed,at={(axis description cs:0.95,0.05)},anchor=west},
height=5cm, width=9cm,
xtick={6,14.87}, ytick=\empty,
xticklabels={$\text{bg median}$,$5\sigma$},
%xticklabels={$\bar n (\theta_t)$},
enlargelimits=false, clip=false,% axis on top,
grid = major]
\addplot [very thick,cyan!20!black,domain=0:20, draw=none] {gammapdf(x,0.2,5)};
\addplot [very thick,cyan!20!black,domain=0.5:19.5] {gammapdf(x,0.2,5)};
%\addplot [domain= 0.5:4.5]{gauss(2.5,0.6,2)};
%\addplot [fill=cyan!20, draw=none, domain=0:6.0] {gammapdf(x,2,2)} \closedcycle;
%\addplot [very thick, fill=white!20!white, draw=none, domain=6.01:20] {gammapdf(x,2,2)} \closedcycle;
\end{axis}
\node at (2.5,1.7) {\begin{tikzpicture} \begin{axis}[hide axis,enlargelimits=false, ymax = 1,height=5cm, width=4cm,domain=2.65:3.35]\addplot [domain= 2.65:3.35,smooth,thick,green]{gauss(3,0.1,0.2)}; \end{axis} \end{tikzpicture}};
\node at (5.5,1.7) {\begin{tikzpicture} \begin{axis}[hide axis,enlargelimits=false, ymax = 1,height=5cm, width=3.5cm,domain=2.7:3.3]\addplot [domain= 2.7:3.3,smooth,thick,red]{gauss(3,0.07,0.14)}; \end{axis} \end{tikzpicture}};
\node[green] at (2.7,3) {$\xrightarrow{\text{90\%}}$};
\node[red] at (6,3) {$\xrightarrow{\text{50\%}}$};
\node at (0.5,3) {$\text{bg}$};
\node[green] at (2.7,4) {$\text{sensitivity}$};
\node[red] at (6,4) {$\text{discovery}$};
\end{tikzpicture}
\caption{Schematic showing the conditions to calculate the sensitivity and discovery potential. The background test statistic is shown in black, the signal test statistic satisfying the sensitivity conditions in green and the one for the discovery potential in red. Note that the curves dont actually look like these shown here. They have been simplified for explanatory purpose.}
\label{fig:sens_disc_schem}
\end{figure}
Let the number of sufficient signal events then be $N_\text{sig}$, the sensitivity $sens$ and discovery potential $disc p$ then are
\begin{align}
  sens &= \frac{N_\text{sig,sens}}{A_\text{det}}E_0, \label{eq:sens}\\
  disc p &= \frac{N_\text{sig,disc p}}{A_\text{det}}E_0, \label{eq:disc}
\end{align}
with the total signal acceptance of the detector $A_\text{det}$ and the reference energy $E_0$.
The time-dependent sensitivity and discovery potential are given in a fluence rather than a flux.% by multiplying with $E_0^2$, these results can then be compared to other analyses.
